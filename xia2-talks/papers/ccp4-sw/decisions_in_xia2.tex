\documentclass[preprint,pdf]{iucr}
\journalcode{D}
\usepackage{graphicx}
\usepackage{cite}

\begin{document}

\title{Decision Making in xia2}

\author[a]{G.}{Winter}
\author[a]{C.}{Lobley}
\author[b]{S.M.}{Prince}
\aff[a]{Diamond Light Source, Harwell Science and Innovation Campus,
Oxfordshire, \country{UK}}
\aff[b]{Faculty of Life Sciences, Manchester Interdisciplinary  Biocentre, 
Manchester, \country{UK}}

\keyword{automation}
\keyword{data reduction}
\keyword{expert system}

\maketitle
\clearpage

\begin{synopsis}
FIXME a synopsis will appear here!
\end{synopsis}

\begin{abstract}

The usage of the data reduction expert system \emph{xia2} was
presented in an earlier article (FIXME CITE.) Here the decision making
protocols employed in the system are presented, with a discussion
of how these were determined, and how they may be applied in interactive
processing. 

\end{abstract}

\section{Introduction}

Decisions made during interactive data reduction are typically based
on experience, advice from program authors and other users. In many
cases this will work, reinforcing the choices made, and it is typically
only when these fail that more detail about those choices is
sought. Equally, suggestions from the program authors rarely include a
systematic appraisal of the possible choices and explanations as to
which options will work best and why. 
In the development of \emph{xia2} this absence was found
to be a limitation giving rise to a systematic investigation of the best
options for a number of programs, 
based on an analysis of data from the Joint Centre
for Structural Genomics\footnote{http://www.jcsg.org} (JCSG.) The
outcomes of this investigation will be described here, along with the
expert system \emph{framework} in which the decision making was
embedded to develop the final data reduction tool, \emph{xia2}.

\subsection{Workflow}

Taken from the top level, the workflow for data reduction for
macromolecular crystallography (MX) can be considered as three phases:
(i) characterisation and (ii) integration of data from individual sweeps,
followed by (iii) scaling and merging of all sweeps taken from a given
crystal. Decisions made at earlier stages has implications for
subsequent stages, and information from the later stages may
contradict those earlier decisions. Any system to automate data
reduction must therefore be sufficiently flexible to manage this
situation gracefully, considering all decisions made as hypotheses
that are subsequently tested. The system must also embody the workflow being
performed to give the structure into which to embed the decision
making expertise.

\subsection{Blueprint}

For any given choice there may be a set of optimum parameters for a
specific example data set. There may however also be a set of
parameters which can be 
shown to generally work well with limited knowledge of the problem in
hand, for example a general ``class'' of data sets.
These \emph{guidelines} may be determined from a systematic
analysis of a number of data sets, which should ideally be free of
artefacts. This criterion highlighted the value of data from 
structural genomics programs, where the published data are well
characterised (i.e. the structure has been solved) and have been
collected in a consistent manner. This has the benefit that the
decision making may focus on the crystallograhic choices, without
needing to work around problems caused by poor sample quality or
inappropriate choices in data collection. 

Finally, it is important to recognise that software for MX is
constantly evolving, and that new packages will become available which
ideally should be incorporated into the system. As such, some emphasis
on abstraction of steps in the workflow is also helpful, to allow
existing components within the system to be replaced. Currently \emph{xia2} 
includes support for two main integration packages, Mosflm
\cite{leslie1992rcm} and XDS \cite{Kabsch:dz5179}
which are accessed as the 2D and 3D
pipelines respectively, reflecting the approach taken to profile
fitting. These are used in combination with Scala \cite{Evans:ba5084}, XSCALE
and more recently Aimless (Evans and Murshudov, this volume) as well
as other
tools from the CCP4 suite to deliver the final result. In
addition, Labelit \cite{Sauter:dd5008} and CCTBX
\cite{Grosse-Kunstleve:ks0118} are also 
extensively used in the analysis.

\section{Decision Making for Data Reduction}

As mentioned above, the crystallographic workflow is considered within
\emph{xia2} in three phases - characterisation, integration and
scaling. Within each of these there will be decisions which need to be
made as well as decisions as to how to handle passing information
around the system, in particular from later stages to earlier. 

Here, the decisions \emph{within} each phase will be considered for
each of the software packages supported, including special cases and
advice the user about appropriate \emph{xia2} options to use where
appropriate.

\subsection{Characterisation}

The objective of the characterisation stage is to determine:

\begin{itemize}
\item{A list of possible Bravais lattice options and appropriate cell
    constants for each.}
\item{Refined / updated values for the beam centre and distance.}
\item{A lattice / cell proposal, based on an analysis of the possible
    options.}
\end{itemize}

\noindent
for a given sweep of images. In addition, any crystal orientation
matrices calculated
should also be available. For a given program, the choices to make
are: the selection of images to use for indexing, the thresholds to
use for indexing, selection of the ``best'' solution and analysis to
ensure that the selected solution appears to be satisfactory. 

\subsubsection{Labelit and Mosflm}

Labelit and Mosflm share the same underlying one-dimensional FFT
indexing algorithm \cite{Steller:mf0013} though the
implementation in Labelit allows for an additional search to refine
the direct beam centre over a wider range. As such, the behaviour of
the programs in indexing is similar, requiring only one analysis for
the selection of images - though the analysis of solutions will depend
on the program being used as they are scored differently.
The authors of both Labelit and Mosflm recommend the use of two images
for indexing, spaced by $\sim 90^{\circ}$ in rotation\footnote{Subsequently
referred to as $\phi$ for brevity} giving a wide coverage of
reciprocal space over which to determine the basis vectors. 

To optimise any process a scoring scheme is needed. Here the objective
is to determine the selection of images which generally give the most
accurate indexing solution. As the R.M.S. deviation between observed
and predicted spot centres is a target of refinement and the ideal absolute
values of the unit cell constants are poorly defined (at least at this
stage) these represent poor metrics. The metric penalty however,
defined as the deviation from the constraints for each Bravais lattice
\cite{Grosse-Kunstleve:sh5006} is appropriate as this is a test for
internal consistency 
and is also relevent for automatic strategy systems such as EDNA
(FIXME CITE.) With this in mind, a secondary constraint is raised:
there is a benefit in using few rather than many images.

\begin{figure}
\caption{Mean normalised metric penalty as a function of number of
  images used for characterisation with Labelit, where smaller values
  indicate more accurate solutions.
\label{figure:no_images}}
\centering
\includegraphics[scale=0.5]{figures/no_images.pdf}
\end{figure}

Data were taken from 86 sweeps from the JCSG archive, where  the
following four
criteria were met: the lattice was not pseudosymmetric, nor triclinic,
autoindexing with a single image gave the ``correct'' result and at
least $90^{\circ}$ of data were available. First, the number of images
to use for indexing was considered. Figure~\ref{figure:no_images}
shows the mean 
normalized metric penalty\footnote{All of the metric penalties for a
  given sweep were scaled over the range $0 - 1$, then this normalized
  value averaged across all sweeps for a given number of images.} for
one to ten images spread across the range $0 - 90^{\circ}$, where
smaller values indicate a more accurate solution. Clearly the use of
two images gives a substantially more accurate result than the use of
a single frame, confirming the advice from the Mosflm and
Labelit authors. However a further improvement may be observed from
using three images, after which no further improvement is clear. 

\begin{figure}
\caption{Normalised metric penalty for indexing from three images, up
  to a maximum spacing of $45^{\circ}$.
\label{figure:phi_spacing_45a}}
\centering
\includegraphics[scale=0.5]{figures/phi_spacing_45a.pdf}
\end{figure}

Following a similar procedure it was found
(Figure~\ref{figure:phi_spacing_45a}) that a 
spacing of $\sim 45^{\circ}$ generally gave the most accurate
solution. Extending this analysis to sweeps of up to $180^{\circ}$
confirmed this result (not shown.) 

This result may be considered as follows: the one-dimensional FFT
indexing procedure employed in Mosflm and Labelit computes the
Fourier transform of the projection of the observed peaks gathered
from all images in $\sim
7000$ reciprocal space directions. Those directions which show the
strongest signal are then considered as possible basis vectors. Using
three images spaced by $\sim 45^{\circ}$ ensures that any reciprocal
space direction is likely to be well sampled. This will make it more likely
that a fundamental basis vector, rather than some linear combination
of basis vectors, will be found giving a more accurate result.

When using Labelit, the proposed solution is chosen to be the highest symmetry
Bravais lattice / cell combination highlighted in the output. The
Bravais lattice / cell combinations for all other lattices are also
recorded, where the cell is chosen to be the one with the lowest
metric penalty where multiple options exist. In usual operation Mosflm
makes a selection from the possible solutions which satisfies similar
constraints, which is taken by \emph{xia2}. Once again, all possible
options are saved. It is important to note that this selection of the
highest symmetry plausible solution is reasonable as it will be
substantially challenged in subsequent analysis steps.

\subsubsection{XDS}

Unlike Mosflm and Labelit, the indexing in XDS uses $\phi$
centroids as well as positions on the image of reflections, and
therefore operates most reliably when given one or more \emph{wedges} of
images. Indeed, it is perfectly possible to index with peaks taken
from every image in the sweep, a process which may be desirable in some
circumstances. The indexing algorithm is however less robust to
errors in the direct beam position than the procedure in
Labelit. The usual procedure in \emph{xia2} is therefore to first
index with Labelit and repeat this indexing with XDS
using the refined beam centre and selected cell and symmetry. However
this behaviour is under the user's control \emph{via} command-line options.

Where Mosflm and Labelit make explicit assumptions about the experimental
geometry, XDS allows a general rotation axis and beam vector and will
typically refine these during the indexing step. Therefore, if the
intention is to use XDS for integration it is necessary to
repeat the indexing of the data if Labelit or Mosflm were used for the
initial characterisation. Determination of the optimum
selection of images for indexing with XDS is therefore necessary.

\begin{figure}
\caption{Mean normalised metric penalty for the correct autoindexing 
solution from XDS, as a function of the number of $5^{\circ}$ wedges used.
The 0 point corresponds to the use of all images. From these results it is
clear that the use of three wedges, rather than the full data set, works
well.
\label{figure:xds_n_images}}
\centering
\includegraphics[scale=0.5]{figures/xds_n_wedges.pdf}
\end{figure}

By anology with Labelit, data were indexed with a triclinic basis
from all images and from one to ten $5^{\circ}$ wedges, with the
resulting triclinic cell used with \verb|iotbx.lattice_symmetry| to
compute the metric penalty for the correct lattice. As may be seen
from Figure~\ref{figure:xds_n_images},
use of a single $5^{\circ}$ wedge gave the poorest
results followed by the use of all images. The use of two and three
wedges gave further improvement, with no improvement observed
subsequently. Following from this, various wedge sizes were tested and
no substantial trends, beyond using at least two frames, were found
though a slight benefit was observed for using $\sim 5^{\circ}$ wedges
(not shown.) Finally the ideal spacing was found to be $\sim
45^{\circ}$ though little improvement was found beyond $\sim
20^{\circ}$ (also not shown.)  

In the case that XDS is used for the initial indexing, it is necessary
to select a proposed solution from the 44 lattice characters output by
XDS. In the analysis no solutions with a penalty higher than 40 were
found to be correct, so with no guidance the system will select the
highest symmetry solution with a penalty lower than this threshold. If
however the symmetry has been given by the user this limit is raised
to 200. As with Labelit, in cases of multiple cell possibilities for a
given lattice the one with the lowest penalty is chosen.

\subsection{Integration}

The objective of the integration step is to:

\begin{itemize}
\item{Accurately measure the intensities of the diffraction spots.}
\item{Verify the results of characterisation, i.e. test whether the
    lattice assigned in the characerisation step is appropriate.}
\item{Provide a refined model for the experimental geometry and
    crystal lattice.}
\end{itemize}

\noindent
While these objectives have been listed in order of decreasing
importance to the structure solution process, the second is perhaps
most significant the development of an expert system. When processing
interactively using for example iMosflm (FIXME CITE) it is
straightforward to see if the indexing solution is appropriate from a
comparison of the observed and predicted reflection positions. When
the analysis is performed with nothing in the way of visual feedback,
as is the case inside \emph{xia2}, an equivalent assessment is
needed. This assessment depends on the program used, and will be
discussed shortly.

\subsubsection{Mosflm}

A typical interactive integration session with Mosflm will start with
indexing followed by the refinement of the cell, once the lattice has
been chosen. At this stage the integration may be performed either
through the GUI or from a script. However, by working through this
process a great deal of information about the data has been learned by
the program, for example spot profle parameters from the spot search
prior to indexing. In automating this analysis, particularly when
alternative programs may be used for some of the steps, some effort
must be made to reproduce the program state. This, coupled with the
cell refinement step, may be characterised as preparation for
integration. The decision making will therefore be separated into
preparation and integration proper.

The preparation for integration is primarily to set up Moslfm into
a suitable state for performing integration. This will require the
selection of images for the cell refinement prior to integration and
the configuration of the program state. To determine rules for the
selection of images a similar process was followed to the selection of
images for indexing with XDS, allowing for an additional constraint of
the use of 30 frames or fewer in cell refinement, with a similar
result. In essence, the cell refinement was performed in P1 for the
sweeps used earlier, and the resulting cell constants scored
\emph{via} metric penalty, resulting in the use of three small wedges
of data spaced by ideally $45^{\circ}$. To prepare the program state
for cell refinement however it is necessary to first gather the
reflection profile parameters that would usually be obtained during
indexing. To achieve this aim an incidental indexing step is
performed, where the spot parameters and an estimate of the resolution
limit are retained but the other results ignored. It was found that
applying this resolution limit during the cell refinement gave rise to
much more reliable results.

To approximate the visual assessment described above, the cell
refinement is performed with the selected lattice from
characterisation and in P1, with the orientation matrix from
characterisation transformed appropriately. When the R.M.S. deviations
between the observed and predicted spot positions are compared in a
pairwise manner (i.e. as a function of frame and cycle) between the
correct lattice and P1 a ratio may be calculated. It was found that in
all cases when the lattice had been correctly assigned this ratio was
less than 1.5. However, in cases where the lattice had pseudosymmetry
the ratio exceeded this value, and is therefore used as a
cutoff. Intuitively this makes sense: if the application of the
lattice constraints makes the aignment of the predictions with the
observed spot positions 50\% worse the lattice constraints are
unlikely to be correct. If this is the case then that solution may be
eliminated from consideration and the next lower symmetry lattice
considered, a process which takes place within the characterisation.
Other tests, comparing the deviations of the refined P1 cell
parameters with the constraints from the lattice, in terms of the
error estimates on those parameters, were found to be unreliable.

Once the integration proper may begin there are a number of choices to
be made:

\begin{itemize}
\item{Whether to fix the cell constants during integration.}
\item{Whether to perform the integration with the lattice constraints
    applied, as recommended by the Mosflm authors.}
\item{Whether to apply a sensible resolution limit during integration,
    as opposed to integrating across the entire active area.}
\end{itemize}

Wheras the choices to date have been assessed in terms of the accuracy
of the resulting cell constants, here the metric will need to relate
to the accuracy of the observations, best observed by scaling the data
and considering the merging statistics. Provided that the extent of
the data are unchanged (i.e. same image range, same resolution limits)
$R_{\rm{merge}}$ will be a reliable indicator of accuracy. To make
this assessment, Pointless (FIXME CITE) and Scala (FIXME CITE) were
used, the latter with a ``standard'' scaling model\footnote{Smoothed
  scaing on rotation with $5^{\circ}$ intervals, secondary absorption
  correction and smoothed $B$ factor correction with $20^{\circ}$
  intervals.}
which is the default in CCP4i and was therefore considered
reasonable. Protocols were determined for integration to assess each
of the earlier choices, by comparison with the procedures recommended
by the program authors. In summary, it was found that fixing the cell
constants during integration was helpful however applying a resolution
cutoff made little difference, as did applying the Bravais lattice
constraints. The conclusion was therefore to integrate across the
active area of the detector, fixing the cell constants and applying
the lattice constraints, though in hindsight there are arguments
against the latter and developments are underway to perform all
processing in P1.

\subsubsection{XDS}

Where Mosflm is typically run through the graphical user interface
iMosflm, XDS can only be run from scripts, making it ideal for usage
within an automated system. The advised use of XDS is to perform all
processing with a triclinic basis, then apply the Bravais lattice
constraints at the final step (i.e. in the postrefinement.) However it
is possible, but by no means mandatory, to ``recycle'' parameters from
the later stages of processing and rerun the integration. The aim here
is to determine the choices which will generally give rise to the best
quality results, ideally at minimal computational cost. As with the
analysis of integration strategies with Mosflm, $R_{\rm{merge}}$ will
be used as a metric for the quality of the data reduction. The four
choices are:

\begin{itemize}
\item{Whether to enforce the Bravais lattice constraints.}
\item{Whether to recycle the reflection profile parameters.}
\item{Whether to recycle the orientation matrix and experimental
    geometry model.}
\item{Whether to recycle all results of postrefinement including local
    detector distortions.}
\end{itemize}

\noindent
These may be compared with the advised protocol stated above. Using
the same data as were used for determination of Mosflm choices, it was
clear that any of these changes to the strategy were polishing the
data, with an average change in $R_{\rm{merge}}$ of around 2\% of the
value. The only marked improvement was found from recycling the
reflection profile parameters, though using the postrefined
orientation matrix etc. seems reasonable, since it will be necessary
to reintegrate the full data set.

In processing the data with Mosflm the selection of Bravais lattice
was tested \emph{via} postrefinement. In XDS, global refinement is
performed after integration allowing the lattice to be tested in a
similar manner, once again with a 50\% threshold. Currently in
\emph{xia2} the integration is performed with the Bravais constraints
applied, for historical reasons, though performing the integration
with a triclinic basis is planned. 

\subsection{Scaling}

While indexing and integration operate on sweeps in isolation,
scaling must consider the data set as a whole. As such, it is at this
point that all of the processed data are brought together and tested
for consistency, both within individual sweeps (i.e. that the symmetry
in intensity data is consistent with the Bravais lattice) and between
sweeps (i.e. that the Bravais lattices are consistent and that the
data are indexed in a consistent manner.) This requires careful
management of derived information as well as the possibiity for feedback to
earlier processing steps. To assist with this the scaling step is
split into three phases - the preparation, the scaling itself and then
post-processing, where for example data sets are combined, structure
factor amplitudes are computed and so forth.

Following the use of Mosflm and XDS, Scala (and now Aimless) and
XSCALE are natural for the main scaling step. However a host of other
programs from CCP4 and elsewhere (including Cad, Truncate and
Pointless) are used to help in this process.

\subsubsection{Preparation}

In the preparation phase the data from integration are tested for
internal consistency both within and between sweeps. The first test is
to determine whether the Bravais lattice used for processing is
consistent with the apparent pointgroup of the data (FIXME FIGURE.)
This test is performed by comparing the highest allowed Bravais
lattice from processing with the most likely result from Pointless. If
the two are consistent (for example, pointgroup P422 with lattice tP)
the test is passed immediately. If the apparent pointgroup has higher
symmetry than the allowed lattice, as may occur when
non-crystallographic symmetry is closely aligned with the unit cell
axes, that pointgroup is ignored and the next most likely tested. If
however the most likely pointgroup has \emph{lower} symmetry than the
Bravais lattice used for processing the latter is eliminated from
consideration as discussed before and the data are reprocessed with
the lattice corresponding to this pointgroup.

Once this test has been passed for all sweeps the data will have been
processed with a lattice consistent with the pointgroup symmetry of
the data. However, it may be the case (for example from a low and high
resolution pass) that the conclusions for one sweep differ from those
from another. In that case (FIXME FIGURE) the lowest symmetry Bravais
lattice is assumed for all sweeps. If at this stage the pointgroup
conclusions remain inconsistent an error is raised as it is likely
that something has gone wrong - for example an inconsistent data set
has been included in the processing. 

Finally, it is necessary to ensure that the data are consistently
indexed to cope with cases where there may be indexing ambiguity,
using Pointless (FIXME FIGURE.) At this stage the most likely
spacegroup is also determined from the systematic absences, primarily
for the benefit of the downstream sructure factor amplitudes, though
often convenient for the program user. When sorting and scaling the
data are put into the order in which they were collected.

\subsubsection{Scaling - Scala}

Though the scaling protocol recommended in the Scala documentation
(smoothed scales over $5^{circ}$ spacing in rotation, absorption
correction with 6 orders of spherical harmonics, $B-$factor correction
smoothed over $20^{circ}$ spacing in rotation) works well in many
cases there are examples when better results may be achieved by
applying for example by including the tails correction or excluding the
decay. As such it is worthwhile to perform a search to decide the most
appropriate model for scaling, which is complicted by the need for a
protocol for decision making. It was found that using Scala
with typical data sets there is essentially no risk of over fitting
the scaling corrections, so the resulting $R_{\rm{merge}}$ was found to
be a reasonable metric as the resolution limits and range of data
considered are unchanged. However, this was found to give an excessive
bias towards corrections for the high resolution data so the low
resolution $R_{\rm{merge}}$ is also taken into account. Finally, in
some cases the combination of scaling corrections may also converge
badly, which is taken to be an indication of a poor model. Therefore,
the resulting protocol is as follows: scaling with only smoothed scale
factors over $5^{circ}$ spacing in rotation gives a reference
convergence rate. Then seven other protocols are tested: with and
without decay, absorption and partiality correction, with the model
which gives the best merging residuals while converging in less than
twice the time of the simple run selected. 

In terms of the parameterisation of the scaling, it was found that for
typical data adjusting the rotation range for the scale and
$B$-factors and the number of spherical harmonics used from the
defaults suggested in the documentation was of little benefit.
Historically, scaling included an iterative process to
determine appropriate correction factors for the error estimates. This
procedure is now performed by Scala and Aimless automatically.

\subsubsection{Scaling - XSCALE}

Where scaling with Scala applies a parameterised model to the data
in order to minimise the differences between symmetry related
observations, scaling in XSCALE (and the XDS CORRECT step) apply
corrections in terms of arrays of correction values which are indexed
appropriately for decay, absorption and detector modulation. In this
scaling, the principle is to remove correlation of the intensity values
with image number, resolution and detector position, 
respectively. The choice of corrections to apply
is determined by the user.

In the XDS CORRECT step these corrections are applied to each sweep in
isolation. The same corrections are applied in XSCALE to all of the
data simultaneously. It was found that performing the scaling twice
gave no improvement but did effectively double the number of
parameters used, so the choice made in \emph{xia2} is to apply all of
the scaling corrections in XSCALE with the minimum used in the XDS
CORRECT step. It was found that the application of all scaling factors
systematically gives the lowest merging residual, so this is the
default choice in \emph{xia2} though a user option is available to
control this choice. In scaling, all of the sweeps corresponding to
each wavelenth are scaled into a single data set but not merged, and
all sweeps are scaled in one run. 

After the initial scaling, resolution limits will be calculated
following the procedures set out below, which are used for subsequent
XSCALE runs. During scaling XSCALE suggests a
list of reflections which do not appear to belong to the data set as a
whole - these are subsequently excluded in an iterative manner.

After scaling the unmerged data are reformatted into their original
sweep structure and merged with Scala or Aimless, as this gives an
easily interpreted report on data quality and also a route into
producing MTZ files for the subsequent post processing, discussed below.

\subsubsection{Resolution Limit Calculation}

Early in the development of \emph{xia2} the resolution limits were
determined from an analysis of the output of Scala, which proved to be
unreliable. Resolution limits are now calculated directly from the
scaled intensity data themselves, using routines developed using
CCTBX, which allows the same procedures to be used for data from Scala
and XSCALE hence giving more consistent results.

The resolution limit calculations themselves use the merged and
unmerged $\frac{I}{\sigma_I}$ values, the completeness and the
$R_{\rm{merge}}$ as possible criteria for determining a cutoff, with
the first two considered by default with values of 2 and 1
respectively. The cutoff applied to the unmerged data was found to be
helpful when considering data with very high multiplicity - though the
multiplicity provides a boost to the apparent $\frac{I}{\sigma_I}$,
the resulting measurements were found to be distributed more like a
normal distrbution than an exponential distribution (i.e. Wilson) as would be
expected, as judged by the $E^4$ plot from Truncate. This is assumed
to be a result of a limited modelling of the correct errors for weak
reflections, giving rise to systematic effects in
merging. $CC\frac{1}{2}$ has since been proposed as a more robust
choice for resolution limit calculation (Evans, private communication)
which will be investigated shortly.

\subsubsection{Post Processing}

From a certain perspective scaled and merged intensities are the end
product of the data integration and scaling. It is however helpful to
perform a small number of subsequent analysis tasks to prepare the
data for downstream analysis:

\begin{itemize}
\item{Calculation of structure factor amplitudes from intensities.}
\item{Determination of an average unit cell.}
\end{itemize}

\noindent
The former of these requires the ``truncate'' procedure (FIXME CITE)
where negative intensities are corrected to give small positive
values. However, to best perform this correction it is necessary to
remove systematically absent reflections from the data set, hence the
spacegroup identification performed earlier, during the preparation
step. At this stage it is also helpful to get the overall scale for
the amplitudes correct - if the amino acid sequence is provided, the
number of molecules in the asymmetric unit is estimated (FIXME CITE
Kanterjeiff and Rupp) to give an estimate of the total number of atoms
and hence the appropriate scale factor to apply. In the absence of
this information it is assumed that the unit cell contains 50\%
solvent, with the remaining space filled with ``average'' protein. 

While the determination of the average unit cell is not a fundamental
part of the analysis, it is nevertheless helpful for downstream
analysis. This is computed in \emph{xia2} as an average from the
sweeps, weighted by the number of reflections in each sweep, hence
giving a bias towards higher resolution data sets.

\section{Implementing Automated Data Reduction}

In the previous section the decision making protocols for the data
analysis were considered. These are not however sufficient to allow
automated data analysis, as they need to be embedded in a framework
which encodes the overall strategy for data analysis. In this section
the structure of \emph{xia2} will be described with an emphasis on the
interaction between the analysis and the data.

\subsection{Data Management}

As an expert system can only make decisions based on the information
available, careful management of data is critical. Most data structures
familiar to MX users deal with static
information, for example coordinate files and reflection data. Here 
the knowledge at any stage is a \emph{hypothesis} and hence
subject to change following subsequent analysis. As such it is
important to track the provenance of any derived information, such
that subsequent invalidation of a result (for example, elimination of
an indexing solution) will automatically invalidate any subsequent
conclusions. This \emph{dynamic} information must be treated carefully
to ensure that results are updated when necessary. This section will
descibe this treatment.

The main information provided to the system is the raw diffraction
data, with suitable metadata (i.e. image headers) to describe the
experiment. In many cases this information will be sufficient to build
a useful model of the experiment. Within \emph{xia2} the raw data are
structured in terms of sweeps of diffraction data, which belong to
wavelengths (which are merged to a single MTZ data set in the output)
which in turn belong to crystals. The crystals are contained within
projects, but this does not currently take part in the analysis. They
are also the fundamental unit of data for scaling. All of these
objects (project, crystal, wavelength, sweep) map directly onto
objects within \emph{xia2} and initially contain only static data -
that is, the information derived from image headers or from user
input. However, starting from this input information it is possible to
perform some analysis as described above, and draw some
conclusions. Although it would be possible to store the results of
this analysis in these data objects, it would be necessary to ensure
that they are kept up-to-date which would generate a substantial
amount of book-keeping. The alternative used inside
\emph{xia2}, is to maintain a link to the
\emph{source} of this information, which is essentially the analysis
task which gave that result. To make this link in a general
way it is necessary to have a standard interface to the analysis
tasks.

\subsection{Expert System Interfaces}

As there are a small number of high level steps in the data analysis
process but a number of possible program options for each, it is
possible to define an abstract interface to describe each step, which
can then be implemented using the existing software 
and the decision making protocols described above. The addition of an
abstract interface also gives the option of centralising standard
decision making steps, for example the management of solutions from
indexing. As an example, both Mosflm and Labelit may be used to index
a diffraction pattern based in a small number of images from a sweep,
so both may present an \emph{Indexer} interface. The detail of how the
indexing is performed and how the results are interpreted will be
program specific and implemented in \emph{MosflmIndexer} and
\emph{LabelitIndexer}, respectively. The application of a standard
interface however allows a standard two-way link to a sweep to be
made, passing in generic information needed for indexing, receiving
generic results from indexing.

While this approach adds an extra degree of separation between the
data and the analysis programs, it allows for generic aspects of the
analysis to be centralised and the freedom to extend, including
new software as it is developed. 

Within each interface (\emph{Indexer} and \emph{Integrater}, linked to
sweeps and 
\emph{Scaler} linked to the crystal) there are three standard phases -
prepare, do and 
finish, arranged in a loop (FIXME FIGURE.) The status of each phase is
managed internally and each phase will be performed until it is
completed or an error occurs. Once the finish phase is completed any
results which have been derived are assumed valid, however in requests
for information the interface will check that this is the case, and if
not will perform the necessary steps. If some input information is
changed this may invalidate the internal state, ensuring that next
time results are requested they are recalculated taking into account
this new information. While this structure is complex, the
real benefit is felt when through the the data management hierarchy.

\subsection{Linking Data Structures and Interfaces}

The benefit to having standard interfaces to all of the analysis
steps, and linking those analysis steps to the data hierarchy, is that
it is clear what the source of a particular piece of information
is. For example, if a sweep is asked for the unit cell and Bravais
lattice it may delegate this request to it's indexer. If the data have
been indexed already and the result not invalidated it may be returned
immediately. If no indexing has been performed this may be done
implicitly, in order to deliver the result. If that result is
subsequently eliminated, the next request for the unit cell will give
the next solution, perhaps repeating the indexing if necessary to
refine this result. 

There are two main outcomes of this. First, in scaling preparation, 
the intensities are requested from integraters and cell and lattice
information from the indexer. If from pointgroup analysis this
lattice is shown to be incorrect it may be eliminated and new
values requested for the cell, lattice and intensities. If necessary
the full integration of the data set will be performed implicitly,
giving the opportunity for sophisticated feedback without excessive
code complexity. The looping structure in each interface will also
ensure that this reprocessing will be performed optimally.
Secondly, the full processing
of the data, from indexing through to scaling of all data from a
crystal, may be performed implicitly by requesting the location of the
final scaled and merged intensity data from the crystal. To this end,
the ``main program'' inside \emph{xia2} is in fact a print statement,
with all of the processing performed as a side-effect of providing the
information to show in the output.

\section{Discussion}

Lots of experts, each with a very small domain of expertise. They are
always consulted, neat coding ensures that if the values are out of
date when requested they are recalculated, otherwise
cached. Ultimately this makes the entire xia2 process a ``print''
statement, showing the merging statistics and the location of the
output reflection files.


\end{document}