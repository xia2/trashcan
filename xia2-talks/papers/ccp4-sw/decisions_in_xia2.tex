\documentclass[a4paper,11pt]{article}
\begin{document}

\section{Abstract}

The usage of the data reduction expert system \emph{xia2} was
presented in an earlier article (FIXME CITE.) Here the decision making
protocols employed in the system will be presented, with a discussion
of how these were determined, and how they may be applied in interactive
processing. 

\section{Introduction}

Decisions made during interactive data reduction are typically based
on experience and advice from program authors and other users. In many
cases this will work, reinforcing the choices made, and it is typically
only when these fail that more detail about those choices is
sought. Equally, suggestions from the program authors rarely include a
systematic appraisal of the possible choices and explanations as to
which options will work best and why. 
In the development of \emph{xia2} this absence was found
as a limitation giving rise to a systematic investigation of the best
options for a number of programs, 
based on an analysis of data from the Joint Centre
for Structural Genomics\footnote{http://www.jcsg.org} (JCSG.) The
outcomes of this investigation will be described here, along with the
expert system \emph{framework} in which the decision making was
embedded to develop the final data reduction tool.

\subsection{Workflow}

Taken from the top level, the workflow for data reduction for
macrmolecular crystallography (MX) is considered as three phases:
characterisation and integration of data from individual sweeps,
followed by scaling and merging of all sweeps taken from a given
crystal. Decisions made at earlier stages will have implications for
subsequent stages, and information from those later stages may
contradict those earlier decisions. Any system to automate data
reduction must therefore be sufficiently flexible to manage this
situation gracefully, considering all decisions made as hypotheses to
be subsequently tested. The system must also embody the workflow to be
performed to give a structure into which to embed the expertise to
make the choices. 

\subsection{Strategy}

For any given choice there may be a set of optimum parameters for a
given case. There may however also be a set of parameters which can be
shown to generally work well with limited knowledge of the problem in
hand. These \emph{guidelines} may be determined from a systematic
analysis of a number of data sets, which should ideally not include
any artefacts. This criterion highlighted the value of data from
structural genomics programs, where the published data are well
characterised (i.e. the structure has been solved) and have been
collected in a consistent manner. This has the benefit that the
decision making may focus on the crystallograhic choices, without
needing to work around problems caused by poor sample quality. 

Finally, it is important to recognise that software for MX is
constantly evolving, and that new packages will become available which
ideally should be incorporated into the system. As such, some emphasis
on abstraction of steps in the workflow is also helpful, to allow
existing components within the system to be replaced. Currently the
system includes support for two main integration packages, Mosflm
(FIXME CITE) and XDS (FIXME CITE) which are accessed as the 2D and 3D
pipelines respectively, reflecting the approach taken to profile
fitting. These are used in combination with Scala (FIXME CITE), XSCALE
(FIXME CITE) and more recently Aimless (FIXME CITE) as well as other
tools from the CCP4 suite (FIXME CITE) to deliver the final result. In
addition, Labelit (FIXME CITE) and CCTBX (FIXME CITE) are also
extensively used in the analysis.

\section{Decision Making for Data Reduction}

As mentioned above, the crystallographic workflow is considered within
\emph{xia2} in three phases - characterisation, integration and
scaling. Within each of these there will be decisions which need to be
made as well as decisions as to how to handle passing information
around the system, in particular from later stages to earlier. 

Here, the decisions \emph{within} each phase will be considered for
each of the software packages supported, including special cases and
advice the user about appropriate program options to use where
appropriate.

\subsection{Characterisation}

The objective of the characterisation stage is to determine:

\begin{itemize}
\item{A list of possible Bravais lattice options and appropriate cell
    constants for each.}
\item{Refined / updated values for the beam centre and distance.}
\item{A lattice / cell selection, based on an analysis of the possible
    options.}
\end{itemize}

\noindent
for a given sweep of images. In addition any $UB$ matrices calculated
should also be available. For a given program, the choices to make
are: the selection of images to use for indexing, the thresholds to
use for indexing, selection of the ``best'' solution and analysis to
ensure that the selected solution appears to be satisfactory. 

\subsubsection{Labelit and Mosflm}

Labelit and Mosflm share the same DPS indexing algorithm, though the
implementation in Labelit allows for an additional search to refine
the direct beam centre over a larger range. As such, the behaviour of
the programs in indexing is similar requiring only one analysis for
the selection of images, though the analysis of solutions will depend
on the program being used, as they are scored differently.
The authors of both Labelit and Mosflm recommend the use of two images
for indexing, spaced by $\sim 90^{\circ}$ in rotation\footnote{Subsequently
referred to as $\phi$ for brevity} giving a wide coverage of
reciprocal space over which to determine the basis vectors. 

To optimise any process a scoring scheme is needed. Here the objective
is to determine the selection of images which generally give the most
accurate indexing solution. As the R.M.S. deviation between observed
and predicted spot centres is a target of refinement and the ideal absolute
values of the unit cell constants are poorly defined (at least at this
stage) these represent poor metrics. The metric penalty however,
defined as the deviation from the constraints for each Bravais lattice
(FIXME CITE) is appropriate as this is a test for internal consistency
and is also relevent for automatic strategy systems such as EDNA
(FIXME CITE.) With this in mind, a secondary constraint is raised:
there is a benefit in using few rather than many images.

Data were taken from 86 sweeps from the JCSG archive, where four
criteria were met: the lattice was not pseudosymmetric, nor triclinic,
autoindexing with a single image gave the ``correct'' result and at
least $90^{\circ}$ of data were available. First, the number of images
to use for indexing was considered. FIXME FIGURE shows the mean
normalized metric penalty\footnote{All of the metric penalties for a
  given sweep were scaled over the range $0 - 1$, then this normalized
  value averaged across all sweeps for a given number of images.} for
one to ten images spread across the range $0 - 90^{\circ}$, where
smaller values indicate a more accurate solution. Clearly the use of
two images gives a substantially more accurate result than the use of
a single frame, as may be expected from the advice from the Mosflm and
Labelit authors, however a further improvement may be observed from
using three images after which no further improvement is clear. 

[FIXME FIGURE]

Following a similar procedure it was found (FIXME FIGURE) that a
spacing of $\sim 45^{\circ}$ generally gave the most accurate
solution. Extending this analysis to sweeps up to $180^{\circ}$
confirmed this result (not shown.) 

[FIXME FIGURE]

This result may be considered as follows: the one-dimensional FFT
indexing procedure employed in Mosflm and Labelit computes the
Fourier transform of the projection of the observed peaks in $\sim
7000$ reciprocal space directions. Those directions which show the
strongest signal are then considered as possible basis vectors. Using
three images spaced by $\sim 45^{\circ}$ ensures that any reciprocal
space direction is likely to be well sampled making it more likely
that a fundamental basis vector, rather than some linear combination
thereof, will be found giving a more accurate result.

When using Labelit, the solution is chosen to be the highest symmetry
Bravais lattice / cell combination highlighted in the output. The
Bravais lattice / cell combinations for all other lattices are also
recorded, where the cell is chosen to be the one with the lowest
metric penalty where multiple options exist. In usual operation Mosflm
makes a selection from the possible solutions which satisfies similar
constraints, which is taken by \emph{xia2}. Once again, all possible
options are saved. It is important to note that this selection of the
highest symmetry plausible solution is reasonable as it will be
substantially challenged in subsequent analysis steps.

\subsubsection{XDS}

Unlike Mosflm and Labelit, the indexing in XDS is able to use $\phi$
centroids as well as image positions of reflections in indexing, and
therefore operates most reliably on one or more \emph{wedges} of
images. Indeed, it is perfectly possible to index with peaks taken
from every image in the sweep, a process which may be desirable in some
circumstances. The indexing algorithm is however less robust against
errors in the direct beam position that the modified DPS procedure in
Labelit. The usual procedure in \emph{xia2} is therefore to first
index with Labelit and, if necessary repeat this indexing with XDS
using the refined beam centre and selected cell and symmetry. 

Where Mosflm makes explicit assumptions about the experimental
geometry, XDS allows a general rotation axis and beam vector and will
typically refine these during the indexing step. Therefore, if the
intention is to use XDS for integration it will be necessary to
repeat the indexing of the data. Determination of the optimum
selection of images for this step is therefore necessary.

By anology with Labelit, data were indexed with a triclinic basis
from all images and from one to ten $5^{\circ}$ wedges, with the
resulting triclinic cell used with \verb|iotbx.lattice_symmetry| to
compute the metric penalty for the correct lattice. As may be seen
from FIXME FIGURE, use of a single $5^{\circ}$ wedge gave the poorest
results followed by the use of all images. The use of two and three
wedges gave further improvement, with no improvement observed
subsequently. Following from this, various wedge sizes were tested and
no substantial trends, beyond using at least two frames, were found
though a slight benefit was observed for using $\sim 5^{\circ}$ wedges
(not shown.) Finally the ideal spacing was found to be $\sim
45^{\circ}$ though little improvement was found beyond $\sim
20^{\circ}$ (also not shown.)  

[FIXME FIGURE]

[FIXME section on selection of indexing solution if XDS is used.]

\subsection{Integration}

The objective of the integration step is to:

\begin{itemize}
\item{Accurately measure the intensities of the diffraction spots.}
\item{Verify the results of characterisation, i.e. test whether the
    lattice assigned in the characerisation step is appropriate.}
\item{Provide a refined model for the experimental geometry and
    crystal lattice.}
\end{itemize}

\noindent
While these objectives have been listed in order of decreasing
importance to the structure solution process, the second is perhaps
most significant the development of an expert system. When processing
interactively using for example iMosflm (FIXME CITE) it is
straightforward to see if the indexing solution is appropriate from a
comparison of the observed and predicted reflection positions. When
the analysis is performed with nothing in the way of visual feedback,
as is the case inside \emph{xia2}, an equivalent assessment is
needed. This assessment depends on the program used, and will be
discussed shortly.

\subsubsection{Mosflm}

A typical interactive integration session with Mosflm will start with
indexing followed by the refinement of the cell, once the lattice has
been chosen. At this stage the integration may be performed either
through the GUI or from a script. However, by working through this
process a great deal of information about the data has been learned by
the program, for example spot profle parameters from the spot search
prior to indexing. In automating this analysis, particularly when
alternative programs may be used for some of the steps, some effort
must be made to reproduce the program state. This, coupled with the
cell refinement step, may be characterised as preparation for
integration. The decision making will therefore be separated into
preparation and integration proper.

The preparation for integration is primarily to set up Moslfm into
a suitable state for performing integration. This will require the
selection of images for the cell refinement prior to integration and
the configuration of the program state. To determine rules for the
selection of images a similar process was followed to the selection of
images for indexing with XDS, allowing for an additional constraint of
the use of 30 frames or fewer in cell refinement, with a similar
result. In essence, the cell refinement was performed in P1 for the
sweeps used earlier, and the resulting cell constants scored
\emph{via} metric penalty, resulting in the use of three small wedges
of data spaced by ideally $45^{\circ}$. To prepare the program state
for cell refinement however it is necessary to first gather the
reflection profile parameters that would usually be obtained during
indexing. To achieve this aim an incidental indexing step is
performed, where the spot parameters and an estimate of the resolution
limit are retained but the other results ignored. It was found that
applying this resolution limit during the cell refinement gave rise to
much more reliable results.

To approximate the visual assessment described above, the cell
refinement is performed with the selected lattice from
characterisation and in P1, with the orientation matrix from
characterisation transformed appropriately. When the R.M.S. deviations
between the observed and predicted spot positions are compared in a
pairwise manner (i.e. as a function of frame and cycle) between the
correct lattice and P1 a ratio may be calculated. It was found that in
all cases when the lattice had been correctly assigned this ratio was
less than 1.5. However, in cases where the lattice had pseudosymmetry
the ratio exceeded this value, and is therefore used as a
cutoff. Intuitively this makes sense: if the application of the
lattice constraints makes the aignment of the predictions with the
observed spot positions 50\% worse the lattice constraints are
unlikely to be correct. If this is the case then that solution may be
eliminated from consideration and the next lower symmetry lattice
considered, a process which takes place within the characterisation.
Other tests, comparing the deviations of the refined P1 cell
parameters with the constraints from the lattice, in terms of the
error estimates on those parameters, were found to be unreliable.

Once the integration proper may begin there are a number of choices to
be made:

\begin{itemize}
\item{Whether to fix the cell constants during integration.}
\item{Whether to perform the integration with the lattice constraints
    applied, as recommended by the Mosflm authors.}
\item{Whether to apply a sensible resolution limit during integration,
    as opposed to integrating across the entire active area.}
\end{itemize}

Wheras the choices to date have been assessed in terms of the accuracy
of the resulting cell constants, here the metric will need to relate
to the accuracy of the observations, best observed by scaling the data
and considering the merging statistics. Provided that the extent of
the data are unchanged (i.e. same image range, same resolution limits)
$R_{\rm{merge}}$ will be a reliable indicator of accuracy. To make
this assessment, Pointless (FIXME CITE) and Scala (FIXME CITE) were
used, the latter with a ``standard'' scaling model\footnote{Smoothed
  scaing on rotation with $5^{\circ}$ intervals, secondary absorption
  correction and smoothed $B$ factor correction with $20^{\circ}$
  intervals.}
which is the default in CCP4i and was therefore considered
reasonable. Protocols were determined for integration to assess each
of the earlier choices, by comparison with the procedures recommended
by the program authors. In summary, it was found that fixing the cell
constants during integration was helpful however applying a resolution
cutoff made little difference, as did applying the Bravais lattice
constraints. The conclusion was therefore to integrate across the
active area of the detector, fixing the cell constants and applying
the lattice constraints, though in hindsight there are arguments
against the latter and developments are underway to perform all
processing in P1.

\subsubsection{XDS}

Where Mosflm is typically run through the graphical user interface
iMosflm, XDS can only be run from scripts, making it ideal for usage
within an automated system. The advised use of XDS is to perform all
processing with a triclinic basis, then apply the Bravais lattice
constraints at the final step (i.e. in the postrefinement.) However it
is possible but by no means mandatory to ``recycle'' parameters from
the later stages of processing and rerun the integration. The aim here
is to determine the choices which will generally give rise to the best
quality results ideally at minimal computational cost. As with the
analysis of integration strategies with Mosflm, $R_{\rm{merge}}$ will
be used as a metric for the quality of the data reduction. The four
choices are:

\begin{itemize}
\item{Whether to enforce the Bravais lattice constraints.}
\item{Whether to recycle the reflection profile parameters.}
\item{Whether to recycle the orientation matrix and experimental
    geometry model.}
\item{Whether to recycle all results of postrefinement including local
    detector distortions.}
\end{itemize}

\noindent
These may be compared with the advised protocol stated above. Using
the same data as were used for determination of Mosflm choices, it was
clear that any of these changes to the strategy were polishing of the
data, with an average change in $R_{\rm{merge}}$ of around 2\% of the
value. The only measurable improvement was found from recycling the
reflection profile parameters, though using the postrefined
orientation matrix etc. seems reasonable, since it will be necessary
to reintegrate the full data set.

In processing the data with Mosflm the selection of Bravais lattice
was tested \emph{via} postrefinement. In XDS, global refinement is
performed after integration allowing the lattice to be tested in a
similar manner, once again with a 50\% threshold. Currently in
\emph{xia2} the integration is performed with the Bravais constraints
applied, for historical reasons, though performing the integration
with a triclinic basis is planned. 

\subsection{Scaling}

Where the indexing and integration operate on sweeps in isolation, the
scaling must consider the data set as a whole. As such, it is at this
point that all of the processed data are brought together and tested
for consistency, both within individual sweeps (i.e. that the symmetry
in intensity data is consistent with the Bravais lattice) and between
sweeps (i.e. that the Bravais lattices are consistent and that the
data are indexed in a consistent manner.) This requires careful
management of derived information as well as the possibiity for feedback to
earlier processing steps. To assist with this the scaling step is
split into three phases - the preparation, the scaling itself and then
post-processing, where for example data sets are combined, structure
factor amplitudes are computed and so forth.

Following the use of Mosflm and XDS, Scala (and now Aimless) and
XSCALE are natural for the main scaling step. However a host of other
programs from CCP4 and elsewhere (for example Cad, Truncate and
Pointless) are used to help in this process.

\subsubsection{Preparation}

In the preparation phase the data from integration are tested for
internal consistency both within and between sweeps. The first test is
to determine whether the Bravais lattice used for processing is
consistent with the apparent pointgroup of the data (FIXME FIGURE.)
This test is performed by comparing the highest allowed Bravais
lattice from processing with the most likely result from pointless. If
the two are consistent (for example, pointgroup P422 with lattice tP)
the test is passed immediately. If the apparent pointgroup has higher
symmetry than the allowed lattice, as may occur when
non-crystallographic symmetry is closely aligned with the unit cell
axes, that pointgroup is ignored and the next most likely tested. If
however the most likely pointgroup has \emph{lower} symmetry than the
Bravais lattice used for processing the latter is eliminated from
consideration as discussed before and the data are reprocessed with
the lattice corresponding to this pointgroup.

Once this test has been passed for all sweeps the data will have been
processed with a lattice consistent with the pointgroup symmetry of
the data. However, it may be the case (for example from a low and high
resolution pass) that the conclusions for one sweep differ from those
from another. In that case (FIXME FIGURE) the lowest symmetry Bravais
lattice is assumed for all sweeps. If at this stage the pointgroup
conclusions remain inconsistent an error is raised as it is likely
that something has gone wrong - for example an inconsistent data set
has been included in the processing. 

Finally, it is necessary to ensure that the data are consistently
indexed to cope with cases where there may be indexing ambiguity,
using Pointless (FIXME FIGURE.) At this stage the most likely
spacegroup is also determined from the systematic absences, primarily
for the benefit of the downstream sructure factor amplitudes, though
often convenient for the program user. When sorting and scaling the
data are put into the order in which they were collected.

\subsubsection{Scaling, with Scala}

Though the scaling protocol recommended in the Scala documentation
(smoothed scales over $5^{circ}$ spacing in rotation, absorption
correction with 6 orders of spherical harmonics, $B-$factor correction
smoothed over $20^{circ}$ spacing in rotation) works well in many
cases there are examples when better results may be achieved by
applying for example the tails correction or not including the
decay. A such it is worthwhile to perform a search to decide the most
appropriate model for scaling, which is complicted by the need for a
protocol for the decision making. It was found that with Scala and
with typical data sets there is essentially no risk of over fitting
the scaling corrections, so the resulting $R_{\rm{merge}}$ was found to
be a reasonable metric as the resolution limits and range of data
considered are unchanged. However, this was found to give an excessive
bias towards corrections for the high resolution data so the low
resolution $R_{\rm{merge}}$ is also taken into account. Finally, in
some cases the combination of scaling corrections may also converge
badly, which is taken to be an indication of a poor model. Therefore,
the resulting protocol is as follows: scaling with only smoothed scale
factors over $5^{circ}$ spacing in rotation gives a reference
convergence rate. Then seven other protocols are tested: with and
without decay, absorption and partiality correction, with the model
which gives the best merging residuals while converging in less than
twice the time of the simple run selected. 

In terms of the parameterisation of the scaling, it was found that for
typical data adjusting the rotation range for the scale and
$B$-factors and the number of spherical harmonics used from the
defaults suggested in the documentation was of little benefit.
Historically the scaling also included an iterative process to
determine approriate correction factors for the error estimates. This
procedure is however now performed by Scala and Aimless automatically.

\subsubsection{Scaling, with XSCALE}

Where the scaling with Scala applies a parameterised model to the data
in order to minimise the differences between symmetry related
observations, the scaling in XSCALE (and the XDS CORRECT step) apply
corrections in terms of arrays of correction values which are indexed
appropriately for decay, absorption and detector modulation. In this
scaling the principle is to remove correlation of the intensity values
with image number and resolution, image number and detector position
and detector position respectively. The choice of corrections to apply
is determined by the user.

In the XDS CORRECT step these corrections are applied to the sweeps in
isolation. The same corrections are applied in XSCALE to all of the
data simultaneously. It was found that performing the scaling twice
gave no improvement but did effectively double the number of
parameters used, so the choice madein \emph{xia2} is to apply all of
the scaling corrections in XSCALE with the minimum used in the XDS
CORRECT step. It was found that the application of all scaling factors
systematically gives the lowest merging residual, so this is the
default choice in \emph{xia2} though a user option is available to
control this choice. In scaling, all of the sweeps corresponding to
each wavelenth are scaled into a single data set but not merged, and
all sweeps are scaled in one run. 

After the initial scaling resolution limits will be calculated
following the procedures set out below, which are used for subsequent
XSCALE runs. During scaling XSCALE suggests a
list of reflections which do not appear to belong to the data set as a
whole - these are subsequently excluded in an iterative manner.

After scaling the unmerged data are reformatted into their original
sweep structure and merged with Scala or Aimless, as this gives an
easily interpreted report on data quality and also a route into
producing MTZ files for the subsequent post processing, discussed below.

\subsubsection{Resolution Limit Calculation}

Early in the development of \emph{xia2} the resolution limits were
determined from an analysis of the output of Scala, which proved to be
unreliable. Resolution limits are now calculated directly from the
scaled intensity data themselves, using routines developed using
CCTBX, which allows the same procedures to be used for data from Scala
and XSCALE hence giving more consistent results.

The resolution limit calculations themselves use the merged and
unmerged $\frac{I}{\sigma_I}$ values, the completeness and the
$R_{\rm{merge}}$ as possible criteria for determining a cutoff, with
the first two considered by default with values of 2 and 1
respectively. The cutoff applied to the unmerged data was found to be
helpful when considering data with very high multiplicity - though the
multiplicity provides a boost to the apparent $\frac{I}{\sigma_I}$,
the resulting measurements were found to be distributed more like a
normal distrbution than an exponential distribution (i.e. Wilson) as would be
expected, as judged by the $E^4$ plot from Truncate. This is assumed
to be a result of a limited modelling of the correct errors for weak
reflections, giving rise to systematic effects in
merging. $CC\frac{1}{2}$ has since been proposed as a more robust
choice for resolution limit calculation (Evans, private communication)
which will be investigated shortly.

\subsubsection{Post Processing}

From a certain perspective scaled and merged intensities are the end
product of the data integration and scaling. It is however helpful to
perform a small number of subsequent analysis tasks to prepare the
data for downstream analysis:

\begin{itemize}
\item{Calculation of structure factor amplitudes from intensities.}
\item{Determination of an average unit cell.}
\end{itemize}

\noindent
The former of these requires the ``truncate'' procedure (FIXME CITE)
where negative intensities are corrected to give small positive
values. However, to best perform this correction it is necessary to
remove systematically absent reflections from the data set, hence the
spacegroup identification performed earlier, during the preparation
step. At this stage it is also helpful to get the overall scale for
the amplitudes correct - if the amino acid sequence is provided, the
number of molecules in the asymmetric unit is estimated (FIXME CITE
Kanterjeiff and Rupp) to give an estimate of the total number of atoms
and hence the appropriate scale factor to apply. In the absence of
this information it is assumed that the unit cell contains 50\%
solvent, with the remaining space filled with ``average'' protein. 

While the determination of the average unit cell is not a fundamental
part of the analysis, it is nevertheless helpful for downstream
analysis. This is computed in \emph{xia2} as an average from the
sweeps, weighted by the number of reflections in each sweep, hence
giving a bias towards higher resolution data sets.

\section{Implementing Automated Data Reduction}

Introduce this section as: we have a set of rules however to make a
working system we also need a framework in which to embed these rules
to express the crystallographic workflow described earlier.

\subsection{Data Management}

Introduce as static data model, how are the inputs
understood. Automated understanding of your data set. Revisit tasks
which can be performed on each, the links to the various steps.

\subsection{Expert System Interfaces}

Introduce this as (i) generally a good idea and (ii) a way of
mediating the link between the data structures and the
processing. Also mention that it means that to some extent the
implementations of these things are interchangable and things which
are in common are only written once.

Also separate out the three steps - prepare, do and finish, then
illustrate how these apply to the three phases of processing.

\subsection{Factories and Links}

Introduce here this idea as a way of a data object accessing
processing functionality about itself, and hold on to that expert as a
source of derived information, and delegate all requests for this
information to the worker.

\section{Discussion}

Lots of experts, each with a very small domain of expertise. They are
always consulted, neat coding ensures that if the values are out of
date when requested they are recalculated, otherwise
cached. Ultimately this makes the entire xia2 process a ``print''
statement, showing the merging statistics and the location of the
output reflection files.

\section{Cruft}

\subsection{Old Notes}

Introduce single-crystal data reduction workflow for MX, identify (i)
the breakdown (ii) the points where decisions need to be made (iii)
the structure of the data (iv) the relationship between this and the
analysis steps. Also be clear about when this becomes more complex,
for example multi-wavelength data sets.

Introduce the scope i.e. the software currently used within \emph{xia2} and
the reasons why these are used.

At this stage also express ``bias'' in terms of choices e.g. generally
minded to do \emph{x} for some value of \emph{x}. At least, if this is
what e.g. Andrew recommends. Certainly should include a summary of
existing program recommendations at this stage.

Introduce the idea of two main ``pipelines'' which differ
predominantly in how they perform the integration and how this by and
large covers the main approaches to the problem. Mention though that
there are no fundamental reasons why more programs could not be
included. Then also make mention of alternative choices along the
pipelines, e.g. 3DII, along with some guidelines as to when these may
be appropriate.

\subsection{Approach}

In this section want to introduce the principles of how to do this -
i.e. deciding which choices generally work well. Basing decision
making choices on data from JCSG, choice of metrics for the decision
making. Also be flexible and sympathetic that the metrics for decision
making could be dependent on the quality of the data. Also parallelism
- bring together multiple threads of processing only when they are
needed, and then ensure that the conclusions drawn were
consistent. State that the alternative, of sharing e.g. orientation
matrices, would be too involved allowing for the potential for feedback.

It is probably important in here to also make mention of limitations
which may result from ths starting point - for example, do the rules
apply to less than ideal, non-structural-genomics data? Also make
mention of limitations of assuming macromolecular data i.e. only 65
spacegroup choices supported. 


\end{document}